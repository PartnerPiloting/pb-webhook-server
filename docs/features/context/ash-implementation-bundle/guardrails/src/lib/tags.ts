export const CANONICAL_TOKENS: string[] = ["sydney","melbourne","brisbane","perth","adelaide","au","nz","product","engineering","partnerships","marketing","data","sales","founder","do-not-contact","qualified","proposal","customer","prospect","churn-risk","linkedin","referral","conference","web-inbound","outbound","smb","mid-market","enterprise","investor","agency","candidate"];
export const SYNONYMS: Record<string,string> = { bris: "brisbane", melb: "melbourne", eng: "engineering", pm: "product", bd: "partnerships", mktg: "marketing", dnc: "do-not-contact" };
export const BANNED: string[] = ["password","secret","token","api-key","credit-card","ssn"];
export const MAX_TOKEN_LENGTH = 32; export const MAX_TOKENS_PER_LEAD = 25; export const TOKEN_RE = /^[a-z0-9]+(-[a-z0-9]+)*(:[a-z0-9]+(-[a-z0-9]+)*)?$/;
export function normalizeRaw(input: string): string { return String(input || "").toLowerCase().normalize("NFKD").replace(/[\u0300-\u036f]/g, "").replace(/[^a-z0-9:\- ,;\n\r\t]+/g, " ").replace(/\s+/g, " ").trim(); }
export function splitTokens(input: string): string[] { return normalizeRaw(input).split(/[,;\s]+/).map(t => t.trim()).filter(Boolean); }
export function canonicalize(token: string): string { return SYNONYMS[token] || token; }
export function validateToken(token: string): { valid: boolean; reason?: string } { if (!token) return { valid:false, reason:"empty" }; if (BANNED.includes(token)) return { valid:false, reason:"banned" }; if (token.length > MAX_TOKEN_LENGTH) return { valid:false, reason:"too-long" }; if (!TOKEN_RE.test(token)) return { valid:false, reason:"bad-format" }; return { valid:true }; }
export function normaliseAndCanonise(raw: string): { tokens: string[]; invalid: string[] } { const out: string[] = []; const bad: string[] = []; const seen = new Set<string>(); for (const rawTok of splitTokens(raw)) { const tok = canonicalize(rawTok); const v = validateToken(tok); if (!v.valid) { bad.push(rawTok); continue; } if (!seen.has(tok)) { seen.add(tok); out.push(tok); } } if (out.length > MAX_TOKENS_PER_LEAD) { return { tokens: out.slice(0, MAX_TOKENS_PER_LEAD), invalid: bad.concat(out.slice(MAX_TOKENS_PER_LEAD)) }; } return { tokens: out, invalid: bad }; }
export function suggest(partial: string, limit = 8): string[] { const p = normalizeRaw(partial); if (!p) return CANONICAL_TOKENS.slice(0, limit); const starts = CANONICAL_TOKENS.filter(t => t.startsWith(p)); const contains = CANONICAL_TOKENS.filter(t => !t.startsWith(p) && t.includes(p)); return [...starts, ...contains].slice(0, limit); }